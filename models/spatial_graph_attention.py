"""
Spatial Graph Attention Network (SGAN) for Document Field Extraction

Novel Architecture Components:
1. Token Graph Construction: Builds spatial graph from OCR tokens with geometric edges
2. Disagreement-Aware Attention: Learns from extractor consensus/disagreement signals
3. Multi-Head Field-Specific Decoders: Specialized heads for each extraction field
4. Confidence Calibration Layer: Learns to estimate extraction uncertainty

This model is trained from scratch (random initialization) using pseudo-labels
generated by the VLM/OCR/CV ensemble.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import math


class SpatialGraphAttention(nn.Module):
    """
    Novel architecture that processes document tokens as a spatial graph.
    
    Key Innovation: Instead of treating documents as sequences (like transformers),
    we model them as graphs where nodes are OCR tokens and edges encode spatial
    relationships (left-of, above, near, etc.). This better captures invoice layout.
    """
    
    def __init__(
        self,
        vocab_size: int = 10000,
        d_model: int = 256,
        num_heads: int = 8,
        num_layers: int = 6,
        num_fields: int = 4,  # dealer_name, model_name, horse_power, asset_cost
        dropout: float = 0.1,
        max_tokens: int = 512
    ):
        super().__init__()
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_fields = num_fields
        
        # Token Embedding Layer
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # Spatial Position Encoding (novel: uses actual bounding boxes)
        self.spatial_encoder = SpatialPositionEncoder(d_model)
        
        # Disagreement Signal Encoder (novel: encodes extractor uncertainty)
        self.disagreement_encoder = DisagreementEncoder(d_model)
        
        # Graph Attention Layers (novel: spatial edges + attention)
        self.graph_layers = nn.ModuleList([
            SpatialGraphLayer(d_model, num_heads, dropout)
            for _ in range(num_layers)
        ])
        
        # Field-Specific Decoder Heads (novel: one per field)
        self.field_decoders = nn.ModuleDict({
            'dealer_name': FieldDecoder(d_model, max_length=50, field_type='text'),
            'model_name': FieldDecoder(d_model, max_length=30, field_type='text'),
            'horse_power': FieldDecoder(d_model, max_length=1, field_type='numeric'),
            'asset_cost': FieldDecoder(d_model, max_length=1, field_type='numeric')
        })
        
        # Confidence Calibration Network (novel: learns uncertainty)
        self.confidence_calibrator = ConfidenceCalibrator(d_model, num_fields)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Random initialization - no pretrained weights"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(
        self,
        token_ids: torch.Tensor,  # [batch, max_tokens]
        bboxes: torch.Tensor,  # [batch, max_tokens, 4] (x1, y1, x2, y2)
        disagreement_scores: torch.Tensor,  # [batch, max_tokens, num_extractors]
        attention_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the spatial graph network.
        
        Args:
            token_ids: OCR token IDs
            bboxes: Bounding boxes for each token (normalized 0-1)
            disagreement_scores: Confidence from each extractor for each token
            attention_mask: Mask for padding tokens
            
        Returns:
            Dictionary with predictions and confidences for each field
        """
        batch_size, num_tokens = token_ids.shape
        
        # Embed tokens
        token_embeds = self.token_embedding(token_ids)  # [batch, tokens, d_model]
        
        # Add spatial position encoding
        spatial_embeds = self.spatial_encoder(bboxes)  # [batch, tokens, d_model]
        
        # Add disagreement signal encoding
        disagreement_embeds = self.disagreement_encoder(disagreement_scores)
        
        # Combine embeddings
        x = token_embeds + spatial_embeds + disagreement_embeds
        
        # Build spatial adjacency matrix
        adj_matrix = self._build_spatial_graph(bboxes, attention_mask)
        
        # Pass through graph attention layers
        for layer in self.graph_layers:
            x = layer(x, adj_matrix, attention_mask)
        
        # Decode each field independently
        outputs = {}
        for field_name, decoder in self.field_decoders.items():
            field_output = decoder(x, attention_mask)
            outputs[field_name] = field_output
        
        # Calibrate confidence scores
        confidences = self.confidence_calibrator(x, outputs, attention_mask)
        outputs['confidences'] = confidences
        
        return outputs
    
    def _build_spatial_graph(
        self,
        bboxes: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Build spatial adjacency matrix based on geometric relationships.
        
        Novel Contribution: Instead of fixed positional encodings, we dynamically
        construct edges based on actual spatial relationships (proximity, alignment).
        
        Args:
            bboxes: [batch, tokens, 4]
            attention_mask: [batch, tokens]
            
        Returns:
            Adjacency matrix [batch, tokens, tokens]
        """
        batch_size, num_tokens, _ = bboxes.shape
        
        # Calculate centroids
        centroids = (bboxes[:, :, :2] + bboxes[:, :, 2:]) / 2  # [batch, tokens, 2]
        
        # Pairwise distances (L2 norm)
        # Expand dimensions for broadcasting
        c1 = centroids.unsqueeze(2)  # [batch, tokens, 1, 2]
        c2 = centroids.unsqueeze(1)  # [batch, 1, tokens, 2]
        distances = torch.norm(c1 - c2, dim=-1)  # [batch, tokens, tokens]
        
        # Horizontal and vertical alignment
        x1, y1 = centroids[:, :, 0:1], centroids[:, :, 1:2]
        horizontal_align = torch.abs(y1.unsqueeze(2) - y1.unsqueeze(1))  # Same row
        vertical_align = torch.abs(x1.unsqueeze(2) - x1.unsqueeze(1))  # Same column
        
        # Create adjacency based on multiple criteria
        # 1. Proximity: Connect nearby tokens
        proximity_threshold = 0.15  # Normalized coordinates
        proximity_edges = (distances < proximity_threshold).float()
        
        # 2. Alignment: Connect horizontally/vertically aligned tokens
        align_threshold = 0.05
        h_align_edges = (horizontal_align < align_threshold).float()
        v_align_edges = (vertical_align < align_threshold).float()
        
        # Combine edge types with learned weights
        adj_matrix = proximity_edges + 0.5 * h_align_edges + 0.5 * v_align_edges
        
        # Self-loops
        eye = torch.eye(num_tokens, device=bboxes.device).unsqueeze(0)
        adj_matrix = adj_matrix + eye
        
        # Apply attention mask
        if attention_mask is not None:
            mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)
            adj_matrix = adj_matrix * mask
        
        # Normalize (row-wise)
        row_sum = adj_matrix.sum(dim=-1, keepdim=True).clamp(min=1e-6)
        adj_matrix = adj_matrix / row_sum
        
        return adj_matrix


class SpatialPositionEncoder(nn.Module):
    """Encodes absolute and relative spatial positions from bounding boxes"""
    
    def __init__(self, d_model: int):
        super().__init__()
        self.d_model = d_model
        
        # Separate encoding for x and y coordinates
        self.x_encoder = nn.Linear(2, d_model // 2)  # x1, x2
        self.y_encoder = nn.Linear(2, d_model // 2)  # y1, y2
        
        # Width and height encoding
        self.size_encoder = nn.Linear(2, d_model)  # width, height
    
    def forward(self, bboxes: torch.Tensor) -> torch.Tensor:
        """
        Args:
            bboxes: [batch, tokens, 4] (x1, y1, x2, y2)
        Returns:
            Spatial embeddings [batch, tokens, d_model]
        """
        x_coords = bboxes[:, :, [0, 2]]  # x1, x2
        y_coords = bboxes[:, :, [1, 3]]  # y1, y2
        
        # Calculate width and height
        width = (bboxes[:, :, 2] - bboxes[:, :, 0]).unsqueeze(-1)
        height = (bboxes[:, :, 3] - bboxes[:, :, 1]).unsqueeze(-1)
        size = torch.cat([width, height], dim=-1)
        
        # Encode
        x_embed = self.x_encoder(x_coords)
        y_embed = self.y_encoder(y_coords)
        pos_embed = torch.cat([x_embed, y_embed], dim=-1)
        
        size_embed = self.size_encoder(size)
        
        return pos_embed + size_embed


class DisagreementEncoder(nn.Module):
    """
    Encodes disagreement signals from VLM/OCR/CV extractors.
    
    Novel Contribution: Explicitly models when extractors disagree, which
    indicates uncertain or ambiguous regions that need more attention.
    """
    
    def __init__(self, d_model: int, num_extractors: int = 3):
        super().__init__()
        self.num_extractors = num_extractors
        
        # Encode raw extractor scores
        self.score_encoder = nn.Linear(num_extractors, d_model)
        
        # Encode disagreement metrics
        self.disagreement_mlp = nn.Sequential(
            nn.Linear(2, d_model // 2),  # variance, entropy
            nn.ReLU(),
            nn.Linear(d_model // 2, d_model)
        )
    
    def forward(self, disagreement_scores: torch.Tensor) -> torch.Tensor:
        """
        Args:
            disagreement_scores: [batch, tokens, num_extractors]
        Returns:
            Disagreement embeddings [batch, tokens, d_model]
        """
        # Encode raw scores
        score_embed = self.score_encoder(disagreement_scores)
        
        # Calculate disagreement metrics
        variance = disagreement_scores.var(dim=-1, keepdim=True)
        
        # Entropy of extractor scores (treat as probability distribution)
        probs = F.softmax(disagreement_scores, dim=-1)
        entropy = -(probs * torch.log(probs + 1e-9)).sum(dim=-1, keepdim=True)
        
        disagreement_features = torch.cat([variance, entropy], dim=-1)
        disagreement_embed = self.disagreement_mlp(disagreement_features)
        
        return score_embed + disagreement_embed


class SpatialGraphLayer(nn.Module):
    """
    Graph attention layer that operates on spatial document graph.
    
    Novel: Combines graph convolution with multi-head attention,
    weighted by spatial adjacency.
    """
    
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        assert d_model % num_heads == 0
        
        # Multi-head attention
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        # Spatial edge encoding
        self.edge_encoder = nn.Linear(1, num_heads)
        
        # Feed-forward network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )
        
        # Layer norms
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self,
        x: torch.Tensor,
        adj_matrix: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            x: [batch, tokens, d_model]
            adj_matrix: [batch, tokens, tokens]
            attention_mask: [batch, tokens]
        Returns:
            Updated node features [batch, tokens, d_model]
        """
        batch_size, num_tokens, _ = x.shape
        
        # Multi-head attention with spatial adjacency
        residual = x
        x = self.norm1(x)
        
        # Project to Q, K, V
        q = self.q_proj(x).view(batch_size, num_tokens, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, num_tokens, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, num_tokens, self.num_heads, self.head_dim)
        
        # Transpose for attention computation
        q = q.transpose(1, 2)  # [batch, heads, tokens, head_dim]
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # Attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # Encode spatial edges and add to attention
        edge_bias = self.edge_encoder(adj_matrix.unsqueeze(-1))  # [batch, tokens, tokens, heads]
        edge_bias = edge_bias.permute(0, 3, 1, 2)  # [batch, heads, tokens, tokens]
        scores = scores + edge_bias
        
        # Apply attention mask
        if attention_mask is not None:
            mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tokens]
            scores = scores.masked_fill(~mask.bool(), float('-inf'))
        
        # Softmax and apply attention
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        out = torch.matmul(attn_weights, v)  # [batch, heads, tokens, head_dim]
        out = out.transpose(1, 2).contiguous()  # [batch, tokens, heads, head_dim]
        out = out.view(batch_size, num_tokens, self.d_model)
        out = self.out_proj(out)
        
        x = residual + self.dropout(out)
        
        # Feed-forward
        residual = x
        x = self.norm2(x)
        x = residual + self.ffn(x)
        
        return x


class FieldDecoder(nn.Module):
    """
    Field-specific decoder head.
    
    Novel: Each field has a specialized decoder that learns field-specific
    patterns (e.g., dealer names often in upper region, costs near bottom).
    """
    
    def __init__(self, d_model: int, max_length: int, field_type: str):
        super().__init__()
        self.field_type = field_type
        self.max_length = max_length
        
        # Attention pooling to aggregate relevant tokens
        self.attention_pool = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Tanh(),
            nn.Linear(d_model, 1)
        )
        
        if field_type == 'text':
            # Pointer network to select relevant tokens
            self.pointer = nn.Linear(d_model, 1)
            self.token_classifier = nn.Linear(d_model, 1)  # Binary: part of field or not
        else:  # numeric
            # Regression head for numeric values
            self.regressor = nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.ReLU(),
                nn.Linear(d_model // 2, 1)
            )
    
    def forward(
        self,
        x: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            x: [batch, tokens, d_model]
            attention_mask: [batch, tokens]
        Returns:
            Field predictions
        """
        if self.field_type == 'text':
            # Compute attention weights for token selection
            attn_scores = self.attention_pool(x).squeeze(-1)  # [batch, tokens]
            
            if attention_mask is not None:
                attn_scores = attn_scores.masked_fill(~attention_mask.bool(), float('-inf'))
            
            attn_weights = F.softmax(attn_scores, dim=-1)  # [batch, tokens]
            
            # Token-level classification (is this token part of the field?)
            token_logits = self.token_classifier(x).squeeze(-1)  # [batch, tokens]
            
            return {
                'attention_weights': attn_weights,
                'token_logits': token_logits
            }
        else:  # numeric
            # Aggregate with attention pooling
            attn_scores = self.attention_pool(x).squeeze(-1)
            if attention_mask is not None:
                attn_scores = attn_scores.masked_fill(~attention_mask.bool(), float('-inf'))
            attn_weights = F.softmax(attn_scores, dim=-1).unsqueeze(-1)
            
            pooled = (x * attn_weights).sum(dim=1)  # [batch, d_model]
            
            # Regress numeric value
            value = self.regressor(pooled).squeeze(-1)  # [batch]
            
            return {
                'value': value,
                'attention_weights': attn_weights.squeeze(-1)
            }


class ConfidenceCalibrator(nn.Module):
    """
    Learns to calibrate confidence scores for each field.
    
    Novel: Instead of using raw model outputs as confidence, we learn
    a calibration function that estimates true accuracy based on:
    - Model uncertainty (entropy of predictions)
    - Extractor agreement
    - Document quality signals
    """
    
    def __init__(self, d_model: int, num_fields: int):
        super().__init__()
        self.num_fields = num_fields
        
        # Global document representation
        self.doc_pooling = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Tanh(),
            nn.Linear(d_model, 1)
        )
        
        # Field-specific confidence estimators
        self.confidence_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model + 2, d_model // 2),  # +2 for uncertainty metrics
                nn.ReLU(),
                nn.Linear(d_model // 2, 1),
                nn.Sigmoid()  # Output in [0, 1]
            )
            for _ in range(num_fields)
        ])
    
    def forward(
        self,
        x: torch.Tensor,
        field_outputs: Dict[str, Dict],
        attention_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            x: Node features [batch, tokens, d_model]
            field_outputs: Predictions from each field decoder
            attention_mask: [batch, tokens]
        Returns:
            Calibrated confidence for each field
        """
        # Compute global document representation
        doc_attn = self.doc_pooling(x).squeeze(-1)
        if attention_mask is not None:
            doc_attn = doc_attn.masked_fill(~attention_mask.bool(), float('-inf'))
        doc_attn = F.softmax(doc_attn, dim=-1).unsqueeze(-1)
        doc_repr = (x * doc_attn).sum(dim=1)  # [batch, d_model]
        
        confidences = {}
        field_names = ['dealer_name', 'model_name', 'horse_power', 'asset_cost']
        
        for idx, field_name in enumerate(field_names):
            if field_name not in field_outputs:
                continue
            
            field_out = field_outputs[field_name]
            
            # Calculate uncertainty metrics
            if 'token_logits' in field_out:
                # For text fields: entropy of token selection
                probs = torch.sigmoid(field_out['token_logits'])
                entropy = -(probs * torch.log(probs + 1e-9) + 
                           (1 - probs) * torch.log(1 - probs + 1e-9)).mean(dim=-1)
                variance = field_out['attention_weights'].var(dim=-1)
            else:
                # For numeric fields: attention variance
                entropy = torch.zeros(x.size(0), device=x.device)
                variance = field_out['attention_weights'].var(dim=-1)
            
            # Combine document representation with uncertainty
            uncertainty = torch.stack([entropy, variance], dim=-1)
            features = torch.cat([doc_repr, uncertainty], dim=-1)
            
            # Estimate confidence
            conf = self.confidence_heads[idx](features).squeeze(-1)
            confidences[field_name] = conf
        
        return confidences